# -*- coding: utf-8 -*-
"""Hallucination demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1exM2BzxIgyIiWxAXj08I3C-bfiW2UN81
"""

import torch
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM

# Model
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name_or_path = "Qwen/Qwen3-0.6B"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)

model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path
)

model.to("cpu")

THINK_TOKEN = 151668

def generate_answer(message, history, memory_depth=5):

    # Keep only recent history
    if memory_depth > 0:
        recent_history = history[-memory_depth:]
    else:
        recent_history = []

    chat = [{"role": "system", "content": "Answer in English"}]

    # Convert Gradio history to chat format
    for user_msg, bot_msg in recent_history:
        if user_msg:
            chat.append({"role": "user", "content": user_msg})
        if bot_msg:
            chat.append({"role": "assistant", "content": bot_msg})

    chat.append({"role": "user", "content": message})

    # Convert to model prompt
    chat_text = tokenizer.apply_chat_template(
        chat,
        tokenize=False,
        add_generation_prompt=True
    )

    # Tokenize
    model_inputs = tokenizer(
        [chat_text],
        return_tensors="pt"
    ).to(model.device)

    # Generate (safe token limit for demo)
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7
    )

    # Extract only new tokens
    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

    # Try to separate thinking vs final answer
    try:
        idx = len(output_ids) - output_ids[::-1].index(THINK_TOKEN)
    except ValueError:
        idx = 0

    answer = tokenizer.decode(
        output_ids[idx:],
        skip_special_tokens=True
    ).strip()

    return answer

# Gradio Interface
with gr.Blocks() as demo:

    gr.Markdown("## AI Chatbot â€” Hallucination Demo")

    memory_slider = gr.Slider(
        minimum=0,
        maximum=20,
        value=5,
        step=1,
        label="Memory Depth"
    )

    chatbot = gr.Chatbot(height=400)

    msg_box = gr.Textbox(
        label="Your Question",
        placeholder="Ask anything..."
    )

    send_btn = gr.Button("Send ðŸš€")


    def process(message, history, memory_depth):
        if not message:
            return history, ""

        answer = generate_answer(message, history, memory_depth)

        history.append((message, answer))

        return history, ""


    send_btn.click(
        process,
        inputs=[msg_box, chatbot, memory_slider],
        outputs=[chatbot, msg_box]
    )


demo.launch(debug=True)